{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import random\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for whatever reason, selenium gets to the end of a page, and keeps trying to click buttons\n",
    "# need to try except this error away\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "# import proxy drivers\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# import async packages\n",
    "import asyncio\n",
    "from proxybroker import Broker\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get \"N_PROXIES\" proxies\n",
    "def get_proxies(N_PROXIES):\n",
    "    # initiate proxy list\n",
    "    proxy_list = []\n",
    "    \n",
    "    # define async function to get proxies\n",
    "    async def show(proxies):\n",
    "        while True:\n",
    "            proxy = await proxies.get()\n",
    "            if proxy is None: break\n",
    "            print('Found proxy: %s' % proxy)\n",
    "            proxy_list.append(proxy)\n",
    "    \n",
    "    # create async loop\n",
    "    proxies = asyncio.Queue()\n",
    "    broker = Broker(proxies)\n",
    "    tasks = asyncio.gather(\n",
    "        broker.find(types=['HTTPS'], limit=N_PROXIES),\n",
    "        show(proxies))\n",
    "    \n",
    "    # run async\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(tasks)\n",
    "    \n",
    "    return proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test a proxy \"TESTS\" times\n",
    "def test_proxy(HOST, PORT, TESTS):\n",
    "    url = \"https://www.rottentomatoes.com\"\n",
    "    \n",
    "    # initialize proxy settings\n",
    "    PROXY = str(HOST) + \":\" + str(PORT)\n",
    "    webdriver.DesiredCapabilities.FIREFOX['proxy'] = {\n",
    "        \"httpProxy\": PROXY,\n",
    "        \"ftpProxy\": PROXY,\n",
    "        \"sslProxy\": PROXY,\n",
    "        \"proxyType\": \"MANUAL\",\n",
    "    }\n",
    "    \n",
    "    # define iter variable, using a flag technique\n",
    "    ITER = 1\n",
    "    \n",
    "    # try to get url, if fail, increase iter\n",
    "    while ITER <= TESTS:\n",
    "        try:\n",
    "            # get the url\n",
    "            driver = webdriver.Firefox()\n",
    "            driver.install_addon('/home/nathanael/Downloads/testdir/noscript/noscript.xpi', temporary=True)\n",
    "            driver.set_page_load_timeout(10)\n",
    "            driver.get(url)\n",
    "            \n",
    "            # let the page load\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # look for ratings to double check if loaded\n",
    "            ratings = driver.find_elements_by_class_name('dynamic-text-list__tomatometer-group')\n",
    "            driver.quit()\n",
    "            \n",
    "            # if successful, set flag to 10, otherwise try again\n",
    "            if ratings:\n",
    "                ITER = 10\n",
    "            else:\n",
    "                ITER += 1\n",
    "        \n",
    "        # failure, increase iter\n",
    "        except:\n",
    "            print(\"failed \" + str(ITER) + \" time(s)\")\n",
    "            ITER += 1\n",
    "            driver.quit()\n",
    "    \n",
    "    # if success, return True, otherwise false\n",
    "    if ITER == 10:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_proxy(proxies):\n",
    "    # make a copy of the list\n",
    "    proxies_copy = proxies\n",
    "    \n",
    "    # use the flag method\n",
    "    flag = 0\n",
    "    while flag == 0:\n",
    "        # choose random index\n",
    "        rand_ind = random.randrange(len(proxies))\n",
    "        proxy = proxies_copy[rand_ind]\n",
    "        \n",
    "        # test this proxy\n",
    "        if test_proxy(proxy.host, proxy.port, 2):\n",
    "            # if successful, return this proxy\n",
    "            flag = 1\n",
    "            return (proxy, proxies_copy)\n",
    "        \n",
    "        # otherwise, remove it\n",
    "        else:\n",
    "            del proxies_copy[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_links gets all links for movies of genre \"genre\" with score between \"score_min\" and \"score_max\"\n",
    "# input \"score_min\", \"score_max\", and \"genre\" as integers\n",
    "def get_links(score_min, score_max, genre, HOST, PORT):\n",
    "\n",
    "    # create url to scrape links from\n",
    "    url = \"https://www.rottentomatoes.com/browse/dvd-streaming-all?\" + \\\n",
    "        \"minTomato=\" + str(score_min) + \"&maxTomato=\" + str(score_max) + \\\n",
    "        \"&services=amazon;hbo_go;itunes;netflix_iw;vudu;amazon_prime;fandango_now&genres=\" + \\\n",
    "        str(genre) + \"&sortBy=release\"\n",
    "\n",
    "    # initialize empty list to put URLs in\n",
    "    endings = []\n",
    "    \n",
    "    # initialize proxy settings\n",
    "    PROXY = str(HOST) + \":\" + str(PORT)\n",
    "    webdriver.DesiredCapabilities.FIREFOX['proxy'] = {\n",
    "        \"httpProxy\": PROXY,\n",
    "        \"ftpProxy\": PROXY,\n",
    "        \"sslProxy\": PROXY,\n",
    "        \"proxyType\": \"MANUAL\",\n",
    "    }\n",
    "\n",
    "    # open the URL\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.install_addon('/home/nathanael/Documents/ublock.xpi', temporary=True)\n",
    "    driver.install_addon('/home/nathanael/Downloads/testdir/noscript/noscript.xpi', temporary=True)\n",
    "    driver.set_page_load_timeout(180)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # look for button to load all the movies\n",
    "    buttons = driver.find_elements_by_class_name('btn.btn-secondary-rt.mb-load-btn')\n",
    "    \n",
    "    # wait five seconds for the page to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # while there are buttons to click, keep clicking them\n",
    "    while(buttons):\n",
    "        try:\n",
    "            buttons[0].click()\n",
    "            \n",
    "            # look for more buttons to click\n",
    "            buttons = driver.find_elements_by_class_name('btn.btn-secondary-rt.mb-load-btn')\n",
    "            time.sleep(2)\n",
    "        except StaleElementReferenceException:\n",
    "            # once we reach bottom of page, break\n",
    "            print(\"Reached bottom of page, scraping links\")\n",
    "            break\n",
    "    \n",
    "    # look for the boxes containing info about the movies\n",
    "    infos = driver.find_elements_by_class_name('movie_info')\n",
    "    for info in infos:\n",
    "        # convert to beautiful soup objects\n",
    "        soup = BeautifulSoup(info.get_attribute('innerHTML'), \"html.parser\")\n",
    "        for link in soup.findAll('a'):\n",
    "            # extract hrefs\n",
    "            endings.append(link.get('href'))\n",
    "\n",
    "    # close the browser once done\n",
    "    driver.quit()\n",
    "    \n",
    "    return endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# same as above, but doesn't use proxies\n",
    "def get_links_no_proxies(score_min, score_max):\n",
    "\n",
    "    # create url to scrape links from\n",
    "    url = \"https://www.rottentomatoes.com/browse/dvd-streaming-all?\" + \\\n",
    "            \"minTomato=\" + str(score_min) + \"&maxTomato=\" + str(score_max) + \\\n",
    "            \"&services=amazon;hbo_go;itunes;netflix_iw;vudu;amazon_prime;fandango_now\" + \\\n",
    "            \"&genres=1;2;4;5;6;8;9;10;11;13;18;14&sortBy=release\"\n",
    "\n",
    "    # initialize empty list to put URLs in\n",
    "    endings = []\n",
    "\n",
    "    # open the URL\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.install_addon('/home/nathanael/Documents/ublock.xpi', temporary=True)\n",
    "    driver.install_addon('/home/nathanael/Downloads/testdir/noscript/noscript.xpi', temporary=True)\n",
    "    driver.set_page_load_timeout(180)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # look for button to load all the movies\n",
    "    buttons = driver.find_elements_by_class_name('btn.btn-secondary-rt.mb-load-btn')\n",
    "    \n",
    "    # wait five seconds for the page to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # while there are buttons to click, keep clicking them\n",
    "    while(buttons):\n",
    "        try:\n",
    "            buttons[0].click()\n",
    "            \n",
    "            # look for more buttons to click\n",
    "            buttons = driver.find_elements_by_class_name('btn.btn-secondary-rt.mb-load-btn')\n",
    "            time.sleep(5)\n",
    "        except StaleElementReferenceException:\n",
    "            # once we reach bottom of page, break\n",
    "            print(\"Reached bottom of page, scraping links\")\n",
    "            break\n",
    "    \n",
    "    # look for the boxes containing info about the movies\n",
    "    infos = driver.find_elements_by_class_name('movie_info')\n",
    "    for info in infos:\n",
    "        # convert to beautiful soup objects\n",
    "        soup = BeautifulSoup(info.get_attribute('innerHTML'), \"html.parser\")\n",
    "        for link in soup.findAll('a'):\n",
    "            # extract hrefs\n",
    "            endings.append(link.get('href'))\n",
    "\n",
    "    # close the browser once done\n",
    "    driver.quit()\n",
    "    \n",
    "    return endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROXIES = 20\n",
    "genres = [1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 18, 14]\n",
    "proxies = get_proxies(N_PROXIES)\n",
    "links_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to scrape by genre, using proxies\n",
    "for genre in genres:\n",
    "    # cycle proxy every genre\n",
    "    (proxy, proxies) = cycle_proxy(proxies)\n",
    "    for i in range(5):\n",
    "        # scrape URL's in sets of 21 score; requires 5 total batches\n",
    "        score_min = i * 21\n",
    "        score_max = min((i + 1) * 21 - 1, 100)\n",
    "        new_links = get_links(score_min, score_max, genre, proxy.host, proxy.port)\n",
    "        \n",
    "        # check if the scrape worked\n",
    "        if new_links:\n",
    "            links_list_2.append(new_links)\n",
    "        else:\n",
    "            print(\"failed \" + str(score_min) + \":\" + str(score_max) + \" of \" + str(genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached bottom of page, scraping links\n",
      "scraped links of score 0\n",
      "scraped links of score 1\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 2\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 3\n",
      "score 6 failed 1 times\n",
      "scraped links of score 4\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 5\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 6\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 7\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 8\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 9\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 10\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 11\n",
      "scraped links of score 12\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 13\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 14\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 15\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 16\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 17\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 18\n",
      "scraped links of score 19\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 20\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 21\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 22\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 23\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 24\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 25\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 26\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 27\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 28\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 29\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 30\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 31\n",
      "scraped links of score 32\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 33\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 34\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 35\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 36\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 37\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 38\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 39\n",
      "scraped links of score 40\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 41\n",
      "scraped links of score 42\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 43\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 44\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 45\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 46\n",
      "scraped links of score 47\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 48\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 49\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 50\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 51\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 52\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 53\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 54\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 55\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 56\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 57\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 58\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 59\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 60\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 61\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 62\n",
      "scraped links of score 63\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 64\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 65\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 66\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 67\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 68\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 69\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 70\n",
      "scraped links of score 71\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 72\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 73\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 74\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 75\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 76\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 77\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 78\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 79\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 80\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 81\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 82\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 83\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 84\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 85\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 86\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 87\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 88\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 89\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 90\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 91\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 92\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 93\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 94\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 95\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 96\n",
      "Reached bottom of page, scraping links\n",
      "scraped links of score 97\n",
      "Reached bottom of page, scraping links\n"
     ]
    }
   ],
   "source": [
    "# there's a glitch on website that doesn't let you see 0 percent movies\n",
    "# initialize links by getting 0 and 1 percent movies\n",
    "links = get_links_no_proxies(0, 1)\n",
    "\n",
    "# script to scrape by score, without proxies\n",
    "for i in range(99):\n",
    "    # using the flag method, give each score three chances to scrape\n",
    "    flag = 1\n",
    "    while(flag <= 3):\n",
    "        # try scraping the URL's; if it fails, increase flag by 1\n",
    "        try:\n",
    "            new_links = get_links_no_proxies(i+2, i+2)\n",
    "        except:\n",
    "            print(\"score \" + str(i+2) + \" failed \" + str(flag) + \" times\")\n",
    "            flag += 1\n",
    "        \n",
    "        # if there are new links, exit the loop; otherwise, increase flag by 1\n",
    "        if new_links:\n",
    "            flag = 10\n",
    "        else:\n",
    "            flag += 1\n",
    "            \n",
    "    # if successful, carry on; otherwise, break\n",
    "    if flag == 10:\n",
    "        links += new_links\n",
    "        print(\"scraped links of score \" + str(i + 2))\n",
    "    else:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('links.pkl', 'wb') as f:\n",
    "    pickle.dump(links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
